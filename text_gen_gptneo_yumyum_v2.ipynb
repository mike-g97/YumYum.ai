{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[{"file_id":"1oVBtDSih_g2eD74nKNhBkxLledh0y8-f","timestamp":1689874592894},{"file_id":"1KMexutfpNDpraKwyX717sJ9t34Imr3ll","timestamp":1687225352751}],"gpuType":"T4"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU","widgets":{"application/vnd.jupyter.widget-state+json":{"ef39e42aa2384146b34a96be9a712e98":{"model_module":"@jupyter-widgets/controls","model_name":"VBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"VBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"VBoxView","box_style":"","children":["IPY_MODEL_ee6c7596727b487f88d1038b04a5aa3c","IPY_MODEL_575a096231244b37bab1ce86169d7195","IPY_MODEL_ec635ae64252423b9b9b823501cdf0e9","IPY_MODEL_e01cf4ce675e46559dfbc363975cc78d"],"layout":"IPY_MODEL_27a850ff675c4885ba27da7ae801553e"}},"5adeae255d5144f5a842f0ca9bb6cff0":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_8a6daa24e90a4fa1a8e43f1e31d72bf2","placeholder":"​","style":"IPY_MODEL_a0e13fd39a0f4b768f774b029973c794","value":"<center> <img\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.svg\nalt='Hugging Face'> <br> Copy a token from <a\nhref=\"https://huggingface.co/settings/tokens\" target=\"_blank\">your Hugging Face\ntokens page</a> and paste it below. <br> Immediately click login after copying\nyour token or it might be stored in plain text in this notebook file. </center>"}},"c1f89fe4232d4bba910ed265bd741052":{"model_module":"@jupyter-widgets/controls","model_name":"PasswordModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"PasswordModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"PasswordView","continuous_update":true,"description":"Token:","description_tooltip":null,"disabled":false,"layout":"IPY_MODEL_0102249074a84c4583dca69669cad3e0","placeholder":"​","style":"IPY_MODEL_e5d8f6bd66724d30a0c6af47615d431b","value":""}},"8e5e029825384abeabf1cd52eb9a8270":{"model_module":"@jupyter-widgets/controls","model_name":"CheckboxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"CheckboxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"CheckboxView","description":"Add token as git credential?","description_tooltip":null,"disabled":false,"indent":true,"layout":"IPY_MODEL_a08c0d2e3c0b4bdd8e9aad1b90b79158","style":"IPY_MODEL_c4898b7f289f4a3f8614541865ce4304","value":true}},"5b24c7b9774446b1a984084b2c8acff4":{"model_module":"@jupyter-widgets/controls","model_name":"ButtonModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ButtonModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ButtonView","button_style":"","description":"Login","disabled":false,"icon":"","layout":"IPY_MODEL_dff12a6b17a248f3a0b056a752cd5216","style":"IPY_MODEL_3c661fdb450544b99db69e4d3cdecba7","tooltip":""}},"98fcee6416b54a40b372ad3e42c5aa7a":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_819fc490dad74035850cef3b773ba67c","placeholder":"​","style":"IPY_MODEL_9228216d7352487d887ce639902c2810","value":"\n<b>Pro Tip:</b> If you don't already have one, you can create a dedicated\n'notebooks' token with 'write' access, that you can then easily reuse for all\nnotebooks. </center>"}},"27a850ff675c4885ba27da7ae801553e":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":"center","align_self":null,"border":null,"bottom":null,"display":"flex","flex":null,"flex_flow":"column","grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":"50%"}},"8a6daa24e90a4fa1a8e43f1e31d72bf2":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"a0e13fd39a0f4b768f774b029973c794":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"0102249074a84c4583dca69669cad3e0":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"e5d8f6bd66724d30a0c6af47615d431b":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"a08c0d2e3c0b4bdd8e9aad1b90b79158":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c4898b7f289f4a3f8614541865ce4304":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"dff12a6b17a248f3a0b056a752cd5216":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"3c661fdb450544b99db69e4d3cdecba7":{"model_module":"@jupyter-widgets/controls","model_name":"ButtonStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ButtonStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","button_color":null,"font_weight":""}},"819fc490dad74035850cef3b773ba67c":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"9228216d7352487d887ce639902c2810":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"2bf91bf4e93b47f6b5f565597bb99ffb":{"model_module":"@jupyter-widgets/controls","model_name":"LabelModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"LabelModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"LabelView","description":"","description_tooltip":null,"layout":"IPY_MODEL_8e1c9e1331604714bf4e73fd4208ecd1","placeholder":"​","style":"IPY_MODEL_9f5a7776f2de4bcc994a4800b1a5a629","value":"Connecting..."}},"8e1c9e1331604714bf4e73fd4208ecd1":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"9f5a7776f2de4bcc994a4800b1a5a629":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"ee6c7596727b487f88d1038b04a5aa3c":{"model_module":"@jupyter-widgets/controls","model_name":"LabelModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"LabelModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"LabelView","description":"","description_tooltip":null,"layout":"IPY_MODEL_34764913393644669fb1ecbffcfa6790","placeholder":"​","style":"IPY_MODEL_010d45f9e3124a5081db628ae9a03dce","value":"Token is valid (permission: write)."}},"575a096231244b37bab1ce86169d7195":{"model_module":"@jupyter-widgets/controls","model_name":"LabelModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"LabelModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"LabelView","description":"","description_tooltip":null,"layout":"IPY_MODEL_3fded44e66db481882b1a6ac3671eab2","placeholder":"​","style":"IPY_MODEL_50e7f5af000f47a89e6db5f1ec700ffe","value":"Your token has been saved in your configured git credential helpers (store)."}},"ec635ae64252423b9b9b823501cdf0e9":{"model_module":"@jupyter-widgets/controls","model_name":"LabelModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"LabelModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"LabelView","description":"","description_tooltip":null,"layout":"IPY_MODEL_57db20e13dcb4e898a31d9f887a67756","placeholder":"​","style":"IPY_MODEL_c03759e6bf5742cd8fb1c932e75bc777","value":"Your token has been saved to /root/.cache/huggingface/token"}},"e01cf4ce675e46559dfbc363975cc78d":{"model_module":"@jupyter-widgets/controls","model_name":"LabelModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"LabelModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"LabelView","description":"","description_tooltip":null,"layout":"IPY_MODEL_b4ac078ddcaa46e7a788808fa5ced40d","placeholder":"​","style":"IPY_MODEL_15aaed0aa87e40f5bf130d35b6118804","value":"Login successful"}},"34764913393644669fb1ecbffcfa6790":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"010d45f9e3124a5081db628ae9a03dce":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"3fded44e66db481882b1a6ac3671eab2":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"50e7f5af000f47a89e6db5f1ec700ffe":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"57db20e13dcb4e898a31d9f887a67756":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c03759e6bf5742cd8fb1c932e75bc777":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"b4ac078ddcaa46e7a788808fa5ced40d":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"15aaed0aa87e40f5bf130d35b6118804":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"b23ae9782f144224a860736d70a59d35":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_38ce10041ebf4ff88cf1595006a69acd","IPY_MODEL_7b51867613894df1a6664924a1724900","IPY_MODEL_b0b0ac0c41fd4ab0a3510bfeec7bf68c"],"layout":"IPY_MODEL_587f4c102ab245af999ef6558f21fddf"}},"38ce10041ebf4ff88cf1595006a69acd":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_298dd37ec6464a30ba1376d1d2ab2308","placeholder":"​","style":"IPY_MODEL_b94b78db5e7f456a9ad923fc5d0c24f2","value":"pytorch_model.bin: 100%"}},"7b51867613894df1a6664924a1724900":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_9935d3b74701443da6bf4d10432bc8e7","max":551192941,"min":0,"orientation":"horizontal","style":"IPY_MODEL_cbdac6bea0d646cca744234770fd6e96","value":551192941}},"b0b0ac0c41fd4ab0a3510bfeec7bf68c":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_6a02441956274469b86ab355a0207768","placeholder":"​","style":"IPY_MODEL_46bae19dee81449bb2969230c279acba","value":" 551M/551M [00:21&lt;00:00, 25.9MB/s]"}},"587f4c102ab245af999ef6558f21fddf":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"298dd37ec6464a30ba1376d1d2ab2308":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"b94b78db5e7f456a9ad923fc5d0c24f2":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"9935d3b74701443da6bf4d10432bc8e7":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"cbdac6bea0d646cca744234770fd6e96":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"6a02441956274469b86ab355a0207768":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"46bae19dee81449bb2969230c279acba":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"35c1ff6acfd1454a9437e763fe86c5b2":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_edb3ef2f57274369a8afa020ffc826ee","IPY_MODEL_4f8e5163ba284a538c6a02027bdd3248","IPY_MODEL_269fcea72f9547eaa074e6c581165c17"],"layout":"IPY_MODEL_68185049c31e46fb8565bc3502ece635"}},"edb3ef2f57274369a8afa020ffc826ee":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_0a38f9f1d01b42cb8925332bc4cea68b","placeholder":"​","style":"IPY_MODEL_e204716817eb43c0bbc377dd71b62c86","value":"training_args.bin: 100%"}},"4f8e5163ba284a538c6a02027bdd3248":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_69148528a7a8477fada1804f0938191d","max":3835,"min":0,"orientation":"horizontal","style":"IPY_MODEL_ad30201c2d00466dbe11c4fd8a7c1c24","value":3835}},"269fcea72f9547eaa074e6c581165c17":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_ad9bb05246de465fb2a7287e95592650","placeholder":"​","style":"IPY_MODEL_a7ebac271ca34936b9e8de23239e8198","value":" 3.83k/3.83k [00:00&lt;00:00, 25.2kB/s]"}},"68185049c31e46fb8565bc3502ece635":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"0a38f9f1d01b42cb8925332bc4cea68b":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"e204716817eb43c0bbc377dd71b62c86":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"69148528a7a8477fada1804f0938191d":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ad30201c2d00466dbe11c4fd8a7c1c24":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"ad9bb05246de465fb2a7287e95592650":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"a7ebac271ca34936b9e8de23239e8198":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}}},"cells":[{"cell_type":"markdown","source":["# Text Generation using GPT (Using Huggingface)"],"metadata":{"id":"KoNqC9E6K5dl"}},{"cell_type":"markdown","source":["## Project Setup"],"metadata":{"id":"BSqHIq-o5c6a"}},{"cell_type":"markdown","source":["## Note:\n","transformers is a python library for implementing transformers arechetcture neural networds on huggin face, by defualt it shouldnt be in oyour python library , so the below command should help you install the package into your notebook session."],"metadata":{"id":"hL0vV5Nnh4_L"}},{"cell_type":"code","source":["!pip install -q transformers"],"metadata":{"id":"eMksH89z5emh"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Note:\n","\n","below are some imports we might be needing for pre-processing(converting to tokens) and torch pytorch (another Neural Network libray) that we might be needing later.\n","\n","google lib is for some operations to interact with your google drive"],"metadata":{"id":"mDm9ZvD9iJWU"}},{"cell_type":"code","source":["import torch\n","import shutil\n","from torch.utils.data import Dataset, random_split\n","from transformers import Trainer, TrainingArguments, GPTNeoForCausalLM, GPT2Tokenizer\n","\n","\n","from google.colab import drive\n"],"metadata":{"id":"6UPda3LTYYoo"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Data Preparation"],"metadata":{"id":"AoZgQSdSedqa"}},{"cell_type":"markdown","source":["## Note:\n","! is magic function , that would run a \"shell\" command.\n","the below command downloads the text file that exists on Github into the python session."],"metadata":{"id":"z0CDgvoLixp6"}},{"cell_type":"code","source":["# Load data into colab\n","!wget https://raw.githubusercontent.com/dsirepos/yumyum/main/recipes_13july_v2.txt"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"DWZAkEC3eicq","executionInfo":{"status":"ok","timestamp":1689288553376,"user_tz":240,"elapsed":568,"user":{"displayName":"Rahul Reddy Vemparala","userId":"16442365183898895721"}},"outputId":"e65380f3-29b9-4415-f8bc-a038d14a2ca0"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["--2023-07-13 22:49:07--  https://raw.githubusercontent.com/dsirepos/yumyum/main/recipes_13july_v2.txt\n","Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n","Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 1009918 (986K) [text/plain]\n","Saving to: ‘recipes_13july_v2.txt’\n","\n","recipes_13july_v2.t 100%[===================>] 986.25K  --.-KB/s    in 0.02s   \n","\n","2023-07-13 22:49:08 (47.7 MB/s) - ‘recipes_13july_v2.txt’ saved [1009918/1009918]\n","\n"]}]},{"cell_type":"markdown","source":["## Note:\n","below function is a \"File system\" operation. where we are making a connection with your google drive account.\n","\n","\n","this should prompt you to give permissions to your google drive, give them.\n","NO need to run this everytime during testing ,should be fine for first execution."],"metadata":{"id":"Z9R2hJmsi_Dv"}},{"cell_type":"code","source":["# Connects colab to google drive\n","drive.mount('/content/drive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"dvfjsVtte5Tm","executionInfo":{"status":"ok","timestamp":1689288558467,"user_tz":240,"elapsed":1634,"user":{"displayName":"Rahul Reddy Vemparala","userId":"16442365183898895721"}},"outputId":"a0d42580-a014-42ec-f72a-adf0b372b332"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}]},{"cell_type":"markdown","source":["##Note:\n","shutil stands for shell utilis , another python library to perfrom some operations similar to shell commands.\n","\n","In this case we are jut copying the text data we downloaded from Github to your Google drive, make sure you have a valid path in your google drive\n"],"metadata":{"id":"R3jkO0vZjbJd"}},{"cell_type":"code","source":["shutil.copy(\"/content/recipes_13july_v2.txt\",\"drive/MyDrive/AICamp/yumyum_v2\")\n"],"metadata":{"id":"IE_gu48efMGH","colab":{"base_uri":"https://localhost:8080/","height":36},"executionInfo":{"status":"ok","timestamp":1689288589339,"user_tz":240,"elapsed":1070,"user":{"displayName":"Rahul Reddy Vemparala","userId":"16442365183898895721"}},"outputId":"6748591b-f0a0-4655-8178-ca0ffc08b58f"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'drive/MyDrive/AICamp/yumyum_v2/recipes_13july_v2.txt'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":110}]},{"cell_type":"markdown","source":["## Note:\n","define a function , to read text data from a file and store as list of lines."],"metadata":{"id":"ZHapp2aij6OY"}},{"cell_type":"code","source":["file_path = \"/content/drive/MyDrive/AICamp/yumyum_v2/recipes_13july_v2.txt\"\n","\n","with open(file_path,'r',encoding='utf-8', errors='' ) as f:\n","  text_corpus = f.read()\n","\n","\n","recipes = text_corpus.replace('>>', ': ').split('\\n\\n')\n","\n"],"metadata":{"id":"el_C875YhzPd"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Print data:"],"metadata":{"id":"3cWGrfKMkGi4"}},{"cell_type":"code","source":["recipes[:4]\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"hJZU6FAekJDH","executionInfo":{"status":"ok","timestamp":1689288630689,"user_tz":240,"elapsed":230,"user":{"displayName":"Rahul Reddy Vemparala","userId":"16442365183898895721"}},"outputId":"3b864bf2-bca6-4e47-a4f7-6f849025b1c8"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["['give me recipe for Cheeseburger Potato Soup:\\n Wash potatoes,\\n prick several times with a fork,\\n Microwave them with a wet paper towel covering the potatoes on high for 6-8 minutes,\\n The potatoes should be soft ready to eat,\\n Let them cool enough to handle,\\n Cut in half lengthwise,\\n scoop out pulp and reserve,\\n Discard shells,\\n Brown ground beef until done,\\n Drain any grease from the meat,\\n Set aside when done,\\n Meat will be added later,\\n Melt butter in a large kettle over low heat,\\n add flour stirring until smooth,\\n Cook 1 minute stirring constantly  Gradually add milk,\\n cook over medium heat stirring constantly until thickened and bubbly,\\n Stir in potato ground beef salt pepper 1 cup of cheese 2 tablespoons of green onion and 1/2 cup of bacon,\\n Cook until heated (do not boil),\\n Stir in sour cream if desired,\\n cook until heated (do not boil),\\n Sprinkle with remaining cheese bacon and green onions ,\\nNER:sour cream bacon pepper extra lean ground beef cheddar cheese green onion baking potatoes milk butter salt.',\n"," 'give me recipe for One-Rise Monkey Bread:\\n Grease a 12-cup fluted tube pan or a 10-inch tube pan,\\n In large bowl blend 1 1/2 cups flour sugar salt yeast water margarine and egg at low speed until moistened,\\n Beat three minutes at medium speed,\\n Stir in remaining flour by hand,\\n Knead dough on floured surface until smooth about 1 minute,\\n Press or roll dough to 15 x 12-inch rectangle,\\n Using sharp knife cut dough into diamond-shaped pieces by cutting into 2 inch strips diagonally across dough,\\n In shallow pan melt 1/3 cup butter,\\n Dip each piece of dough in melted butter and place in prepared pan making layers  Sprinkle each layer with poppy seeds,\\n Cover,\\n let rise in warm place until light and doubled in size 45 to 60 minutes,\\n Heat oven to 375 degrees Fahrenheit,\\n Bake 20 to 25 minutes,\\n Cool in pan 2 minutes,\\n Invert into serving pan,\\n Makes a 10-inch pull apart loaf ,\\nNER:bread flour egg sugar salt very warm water butter active dry yeast.',\n"," 'give me recipe for Veal With Mushrooms And Peppers:\\n Preheat oven to 350 degrees Fahrenheit,\\n Heat 2 tablespoons oil in heavy large ovenproof skillet over medium-high heat,\\n Dredge veal in flour shaking off excess,\\n Add veal to skillet and cook until brown stirring occasionally (about 5 minutes),\\n Transfer veal to plate using slotted spoon,\\n Heat remaining 2 tablespoons olive oil in same skillet,\\n Add mushrooms bell pepper oregano allspice and crushed red pepper to skillet,\\n Saute until mushrooms are just beginning to brown (about 3 minutes),\\n Stir in Marsala wine and garlic and boil until skillet is almost dry,\\n Add veal and juices on plate to skillet,\\n Mix in chicken broth and bring to a boil  Cover and bake in oven until veal is tender,\\n Transfer to stove and boil until liquid thickens,\\n Serve with rice,\\n Serves 4 ,\\nNER:low-salt chicken broth veal stew meat oregano bell pepper red pepper mushrooms flour ground allspice steamed white rice garlic marsala wine olive oil.',\n"," 'give me recipe for Lemon Shrimp Oriental:\\n Thaw shrimp if frozen and drain well,\\n Peel and devein shrimp,\\n For sauce stir together water cornstarch soy sauce sugar bouillon granules lemon peel and juice and pepper,\\n Set aside,\\n Preheat a wok or large skillet over high heat,\\n Add 1 tablespoon of the oil,\\n Add mushrooms celery and green pepper,\\n stir-fry for 3 minutes,\\n Add green onion and stir-fry for 1 minute more,\\n Remove vegetables from wok,\\n Add remaining oil,\\n Add half of shrimp to hot wok,\\n Stir-fry 2 to 3 minutes or until shrimp turn pink,\\n Remove and stir-fry remaining shrimp,\\n Return shrimp to wok,\\n Push from center,\\n Stir sauce,\\n Add to center of the wok  Cook and stir until thickened and bubbly,\\n Add vegetables and pea pods,\\n Stir to coat mixture with sauce,\\n Cook and stir for 1 minute,\\n Serve over rice ,\\nNER:sugar instant chicken soy sauce green onions lemon juice pepper stalks celery cooking oil mushrooms water cornstarch frozen pea pods rice shrimp lemon peel green pepper.']"]},"metadata":{},"execution_count":113}]},{"cell_type":"markdown","source":["## Note:\n","'\\n' is an escape sequence , which seems weird, but it takes the cursor to a new line."],"metadata":{"id":"rfjnI9JCkaK8"}},{"cell_type":"markdown","source":["## Note:\n","1. the below code, removes unnecessary start and end words. its a simple list slicing , where we are starting out with a thrid sentence and going upto last line.\n","2. and then removing lines with length zero , which means empty lines."],"metadata":{"id":"2Z8G1Z8ck9BB"}},{"cell_type":"markdown","source":["## Note:\n","TOkenization : https://huggingface.co/docs/transformers/\n","\n","Below is a class in python , takes a couple of arguements at instanciation.\n","1. txt list : sentences\n","2. tokenizer : tokenizer to be used .\n","3. max_length : max length of input\n","\n","\n","3. this should encode the input sentences using the tokenizer and create 3 instance variables named\n","1. input_ids : index for the tokens\n","2. attention_mask: flag that represents a specigic token is important or not\n","3. labels: ?"],"metadata":{"id":"0ZKbCHxQl4k7"}},{"cell_type":"code","source":["# Custome dataset class to load dataset\n","class RecipeDataset(Dataset):\n","    def __init__(self, txt_list, tokenizer, max_length):\n","        self.input_ids = []\n","        self.attn_masks = []\n","        self.labels = []\n","        for txt in txt_list:\n","            # Encode the descriptions using the GPT-Neo tokenizer\n","            encodings_dict = tokenizer('<|startoftext|>'\n","                                        + txt +\n","                                        '<|endoftext|>',\n","                                        truncation=True,\n","                                        max_length=max_length,\n","                                            padding=\"max_length\")\n","            input_ids = torch.tensor(encodings_dict['input_ids'])\n","            self.input_ids.append(input_ids)\n","            mask = torch.tensor(encodings_dict['attention_mask'])\n","            self.attn_masks.append(mask)\n","\n","    def __len__(self):\n","        return len(self.input_ids)\n","\n","    def __getitem__(self, idx):\n","        return self.input_ids[idx], self.attn_masks[idx]"],"metadata":{"id":"FRVhqfCSgIOt"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Initialize tokenizer, model"],"metadata":{"id":"YyWp2-T5jU1D"}},{"cell_type":"markdown","source":["## Note:\n","Steps being implemented below:\n","1.  manual_Seed(42) , a seed used when there is a deterministic randomization in any process. OFten the arechtecture of neural networks where the weights are adjsuted automatically . so seed helps to reproduce the same result, the word 42 is specically telling to reproduce results.\n","\n","2. we are creating a tokenizer instance , where we are giving a few arguments,\n","  a. name of the pretrained tokenizer on hgginf face , 'user_id/model_name'\n","  b. bos_token: start of your input text data\n","  c. eos_token: end of your input text\n","  d. pad : often times the sequences( phrases or sentences are of variable length, internal when feeding it to the neural networks it has to be of fixed length, pad token is used to fill in if the length is less than the expected legnth for the arechetecture)\n","\n","4. initiallize model , name of the model, and cuda function to tell colab to execute the training on gpus\n","\n","5. model resize to the tokens additionally added"],"metadata":{"id":"RjaLARipnWIs"}},{"cell_type":"code","source":["# Set the random seed to a fixed value to get reproducible results\n","torch.manual_seed(42)\n","\n","# Download the pre-trained GPT-Neo model's tokenizer\n","# Add the custom tokens denoting the beginning and the end\n","# of the sequence and a special token for padding\n","tokenizer = GPT2Tokenizer.from_pretrained(\"EleutherAI/gpt-neo-125M\",\n","                            bos_token='<|startoftext|>',\n","                            eos_token='<|endoftext|>',\n","                            pad_token='<|pad|>')\n","\n","# Download the pre-trained GPT-Neo model and transfer it to the GPU\n","model = GPTNeoForCausalLM.from_pretrained(\"EleutherAI/gpt-neo-125M\").cuda()\n","\n","# Resize the token embeddings because we've just added 3 new tokens\n","model.resize_token_embeddings(len(tokenizer))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"lwve_LeijIrB","executionInfo":{"status":"ok","timestamp":1689288646941,"user_tz":240,"elapsed":4606,"user":{"displayName":"Rahul Reddy Vemparala","userId":"16442365183898895721"}},"outputId":"878bbbe2-8dcc-4882-d15d-e5daebbf24b6"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"]},{"output_type":"execute_result","data":{"text/plain":["Embedding(50259, 768)"]},"metadata":{},"execution_count":115}]},{"cell_type":"markdown","source":["## Train/Test Split data"],"metadata":{"id":"mnaVbPqAl6pJ"}},{"cell_type":"markdown","source":["## Note:\n","Standard practice in ML , where we split our input data into 3 parts\n","1. train data : 70% of the total input\n","2. test data : 15%\n","3. validation data : 15%\n","\n","these numbers are not a fixed rule to be used everytime, but changes depending on the problem being solved and size of data, there is no definitive rule to choosing the ratios for spliting data. but often times the above ratios are used.\n"],"metadata":{"id":"hPnxA33Kohel"}},{"cell_type":"code","source":["# subset\n","\n","subset = [x for x in recipes if len(tokenizer.encode(x)) < 260 and len(tokenizer.encode(x)) > 220 ]\n","\n","subset  = subset[:1000]\n","\n"],"metadata":{"id":"jDHredrTmsC2"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["\n","max_length = max([len(tokenizer.encode(recipe)) for recipe in recipes])\n","# min_length = min([len(tokenizer.encode(recipe)) for recipe in recipes])\n","# avg_length = sum([len(tokenizer.encode(recipe)) for recipe in recipes])/len(recipes)\n","\n","# Load dataset\n","dataset = RecipeDataset(recipes, tokenizer, max_length)\n","\n","# Split data into train/val\n","train_size = int(0.8 * len(dataset))\n","\n","train_data, val_data = random_split(dataset, [train_size, len(dataset) - train_size])\n","\n","# max_length, min_length, avg_length"],"metadata":{"id":"azAXcv50l9_S"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(f\"Max sequence length : {max_length}\")\n","print(f\"training set length : {len(train_data)}\")\n","print(f\"validation set length : {len(val_data)}\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"lEUoTaf-ovzq","executionInfo":{"status":"ok","timestamp":1689288691943,"user_tz":240,"elapsed":236,"user":{"displayName":"Rahul Reddy Vemparala","userId":"16442365183898895721"}},"outputId":"8235623e-6fd3-48be-c15c-e6979448e32f"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Max sequence length : 270\n","training set length : 801\n","validation set length : 201\n"]}]},{"cell_type":"markdown","source":["## Tokenizer functions :\n","\n","tokenizer typically should have 2 functions\n","1. encode : convert the text data into tokens\n","2. decode : put back in the original text form.\n","\n"," a more detailed explaination in the below link:\n"," https://huggingface.co/docs/transformers/preprocessing"],"metadata":{"id":"Rl7IIi9Npu5J"}},{"cell_type":"code","source":["tokenizer.batch_decode(val_data[0])"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"JMKRswOTof4K","executionInfo":{"status":"ok","timestamp":1689288700786,"user_tz":240,"elapsed":242,"user":{"displayName":"Rahul Reddy Vemparala","userId":"16442365183898895721"}},"outputId":"344e3b37-0546-4b2d-edec-862e80453b4f"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[\"<|startoftext|> give me recipe for Allegro'S Stuffed Green Peppers:\\n Mix water bulgur onion salt and garlic powder in 2-quart casserole,\\n Cover tightly and microwave on High 6 to 8 minutes or until boiling,\\n stir,\\n Cover and let stand until water is absorbed about 10 minutes,\\n Cut thin slice from stem end of each bell pepper,\\n Remove seeds and membranes,\\n rinse,\\n Arrange peppers cut ends up in circle in pie plate (9 x 1 1/4 or 10 x 1 1/2 inches)  Crumble ground beef into bulgur mixture,\\n stir in 1 cup of the tomato sauce,\\n Fill each pepper with about 1/2 cup mixture,\\n Pour remaining tomato sauce over peppers,\\n Cover tightly and microwave on High 12 to 14 minutes rotating pie plate 1/2 turn after 7 minutes until beef mixture is done (160 degrees Fahrenheit on meat thermometer)  Sprinkle peppers with cheese,\\n let stand uncovered 5 minutes,\\nNER:bulgur hot water green bell peppers cheddar cheese onion garlic powder tomato sauce ground beef salt.<|endoftext|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|>\",\n"," '\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!']"]},"metadata":{},"execution_count":118}]},{"cell_type":"markdown","source":["## Train Model"],"metadata":{"id":"tZ_TRgtAmcN1"}},{"cell_type":"markdown","source":["## Note:\n","Training arguemnts : is aclass in transformer python module, that should help you configure your neural network with various parameters:\n","\n","1. output_dir : place to save model check points\n","2. num_train_epochs= 5 epoch is a time variable, where in this context, tells how many times the entire network should loop while trainig ( how many times the input should pass through the network with back propogation)\n","3. logging steps : freq at which infommation has to be logged onto the console for understanding the train process.\n","4. save stps: freq at which check points are saved .\n","5. evaliation strategy : steps ; how the model to be evaluated\n","6. eval steps: freq at which the model trainig pricess has to evaluated\n","7. per device train batch size : batch size for train set  gpu( t4) tensor chip on colab\n","7. per device eval e size : batch size for eval set gpu( t4) tensor chip on colab\n","8. warm up rate:\n","9. learning rate: step size or how big the gradient should be , to avoid exploding gradients or vanishing gradients .\n","10 . weight decay : specify how much weight decay should be applied during back propogration\n","11. loggin dir : directory for training logs ( log informaion is useful information that gives detials during the train process)"],"metadata":{"id":"KRJfx-ESqOvk"}},{"cell_type":"markdown","source":["## Note:\n","Module dependency verison issue, if you encounter this , it should prompt you to run the below magic function , run it and restart your run time , to execute smoothly"],"metadata":{"id":"oWYmriB-t5p7"}},{"cell_type":"code","source":["!pip install accelerate -U\n"],"metadata":{"id":"9bua1YvksMnO","executionInfo":{"status":"ok","timestamp":1689275602986,"user_tz":240,"elapsed":4027,"user":{"displayName":"Rahul Reddy Vemparala","userId":"16442365183898895721"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"05c5b57f-9ca4-4674-da52-be5e9ee5c099"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: accelerate in /usr/local/lib/python3.10/dist-packages (0.21.0)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from accelerate) (1.22.4)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from accelerate) (23.1)\n","Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate) (5.9.5)\n","Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from accelerate) (6.0)\n","Requirement already satisfied: torch>=1.10.0 in /usr/local/lib/python3.10/dist-packages (from accelerate) (2.0.1+cu118)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (3.12.2)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (4.7.1)\n","Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (1.11.1)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (3.1)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (3.1.2)\n","Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (2.0.0)\n","Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.10.0->accelerate) (3.25.2)\n","Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.10.0->accelerate) (16.0.6)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.10.0->accelerate) (2.1.3)\n","Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.10.0->accelerate) (1.3.0)\n"]}]},{"cell_type":"markdown","source":["## Note:\n","I wouldn't recommand the below cell , if your just testing how this works, as it takes siginifcant amount of time to train the model for 3 different learning rates.\n","\n","If your trying out your model for different learning rates, then its fine, otherwise its not wise to run it numerous times.\n","\n","Instead run the below cell, which trains the model only for the best suitable learning rate."],"metadata":{"id":"SRkQ1WTqvmvV"}},{"cell_type":"code","source":["\n","# Here I will pass the output directory where\n","# the model predictions and checkpoints will be stored,\n","# batch sizes for the training and validation steps,\n","# and warmup_steps to gradually increase the learning rate\n","learning_rates = [5e-5, 3e-5, 1e-5]\n","\n","for learning_rate in learning_rates:\n","\n","    training_args = TrainingArguments(output_dir=f'./results_{learning_rate}',\n","                                      num_train_epochs=10,\n","                                      logging_steps=1000,\n","                                      save_steps=1000,\n","                                      evaluation_strategy='steps',\n","                                      eval_steps=1000,\n","                                      per_device_train_batch_size=2,\n","                                      per_device_eval_batch_size=2,\n","                                      warmup_steps=100,\n","                                      learning_rate=learning_rate,\n","                                      weight_decay=0.01,\n","                                      logging_dir=f'./logs_{learning_rate}')\n","\n","    trainer = Trainer(model=model, args=training_args,\n","                      train_dataset=train_data,\n","                      eval_dataset=val_data,\n","                      # This custom collate function is necessary\n","                      # to built batches of data\n","                      data_collator=lambda data: {'input_ids': torch.stack([f[0] for f in data]),\n","                  'attention_mask': torch.stack([f[1] for f in data]),\n","                  'labels': torch.stack([f[0] for f in data])})\n","\n","    # Start training process!\n","    print(f\"Training result for learning rate: {learning_rate}\")\n","    trainer.train()\n","    print(\"\\n\\n\")"],"metadata":{"id":"W9eluje2md8e","colab":{"base_uri":"https://localhost:8080/","height":510},"executionInfo":{"status":"error","timestamp":1689281851724,"user_tz":240,"elapsed":3054,"user":{"displayName":"Rahul Reddy Vemparala","userId":"16442365183898895721"}},"outputId":"24a191d5-74fd-4112-945f-aca0a5049e88"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Training result for learning rate: 5e-05\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n","  warnings.warn(\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","    <div>\n","      \n","      <progress value='11' max='4000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [  11/4000 00:01 < 12:16, 5.42 it/s, Epoch 0.03/10]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Step</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","  </tbody>\n","</table><p>"]},"metadata":{}},{"output_type":"error","ename":"KeyboardInterrupt","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-62-cb5b811bfcc4>\u001b[0m in \u001b[0;36m<cell line: 7>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     31\u001b[0m     \u001b[0;31m# Start training process!\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Training result for learning rate: {learning_rate}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m     \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     34\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\n\\n\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   1643\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_inner_training_loop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_train_batch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauto_find_batch_size\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1644\u001b[0m         )\n\u001b[0;32m-> 1645\u001b[0;31m         return inner_training_loop(\n\u001b[0m\u001b[1;32m   1646\u001b[0m             \u001b[0margs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1647\u001b[0m             \u001b[0mresume_from_checkpoint\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mresume_from_checkpoint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36m_inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   1936\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1937\u001b[0m                 \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maccelerator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maccumulate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1938\u001b[0;31m                     \u001b[0mtr_loss_step\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1939\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1940\u001b[0m                 if (\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mtraining_step\u001b[0;34m(self, model, inputs)\u001b[0m\n\u001b[1;32m   2757\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2758\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompute_loss_context_manager\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2759\u001b[0;31m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompute_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2760\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2761\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_gpu\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mcompute_loss\u001b[0;34m(self, model, inputs, return_outputs)\u001b[0m\n\u001b[1;32m   2782\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2783\u001b[0m             \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2784\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2785\u001b[0m         \u001b[0;31m# Save past state if it exists\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2786\u001b[0m         \u001b[0;31m# TODO: this needs to be fixed and made cleaner later.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1499\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1500\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1502\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1503\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/models/gpt_neo/modeling_gpt_neo.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, past_key_values, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    742\u001b[0m         \u001b[0mreturn_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreturn_dict\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mreturn_dict\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muse_return_dict\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    743\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 744\u001b[0;31m         transformer_outputs = self.transformer(\n\u001b[0m\u001b[1;32m    745\u001b[0m             \u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    746\u001b[0m             \u001b[0mpast_key_values\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpast_key_values\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1499\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1500\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1502\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1503\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/models/gpt_neo/modeling_gpt_neo.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, past_key_values, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    622\u001b[0m                 )\n\u001b[1;32m    623\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 624\u001b[0;31m                 outputs = block(\n\u001b[0m\u001b[1;32m    625\u001b[0m                     \u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    626\u001b[0m                     \u001b[0mlayer_past\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlayer_past\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1499\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1500\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1502\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1503\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/models/gpt_neo/modeling_gpt_neo.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, layer_past, attention_mask, head_mask, use_cache, output_attentions)\u001b[0m\n\u001b[1;32m    326\u001b[0m         \u001b[0mresidual\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhidden_states\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    327\u001b[0m         \u001b[0mhidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mln_1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 328\u001b[0;31m         attn_outputs = self.attn(\n\u001b[0m\u001b[1;32m    329\u001b[0m             \u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    330\u001b[0m             \u001b[0mlayer_past\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlayer_past\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1499\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1500\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1502\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1503\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/models/gpt_neo/modeling_gpt_neo.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, layer_past, attention_mask, head_mask, use_cache, output_attentions)\u001b[0m\n\u001b[1;32m    278\u001b[0m         \u001b[0moutput_attentions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    279\u001b[0m     ):\n\u001b[0;32m--> 280\u001b[0;31m         return self.attention(\n\u001b[0m\u001b[1;32m    281\u001b[0m             \u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    282\u001b[0m             \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1499\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1500\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1502\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1503\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/models/gpt_neo/modeling_gpt_neo.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, layer_past, head_mask, use_cache, output_attentions)\u001b[0m\n\u001b[1;32m    241\u001b[0m             \u001b[0mpresent\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    242\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 243\u001b[0;31m         \u001b[0mattn_output\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattn_weights\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_attn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquery\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhead_mask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    244\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    245\u001b[0m         \u001b[0mattn_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_merge_heads\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mattn_output\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_heads\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead_dim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/models/gpt_neo/modeling_gpt_neo.py\u001b[0m in \u001b[0;36m_attn\u001b[0;34m(self, query, key, value, attention_mask, head_mask)\u001b[0m\n\u001b[1;32m    194\u001b[0m         \u001b[0;31m# Need to be a tensor, otherwise we get error: `RuntimeError: expected scalar type float but found double`.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    195\u001b[0m         \u001b[0;31m# Need to be on the same device, otherwise `RuntimeError: ..., x and y to be on the same device`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 196\u001b[0;31m         \u001b[0mmask_value\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmask_value\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattn_weights\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mattn_weights\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    197\u001b[0m         \u001b[0mattn_weights\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwhere\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcausal_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattn_weights\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmask_value\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}]},{"cell_type":"markdown","source":["BAsed on the results above, it looks like model trained with learning rate = 5e-5 is more promising than others."],"metadata":{"id":"vAbDm1Osohai"}},{"cell_type":"markdown","source":["## Note:\n","The lesser the loss ,the better"],"metadata":{"id":"it3jhQ_vwxDF"}},{"cell_type":"code","source":["training_args = TrainingArguments(output_dir=f'./results',\n","                                      num_train_epochs=5,\n","                                      logging_steps=1000,\n","                                      save_steps=5000,\n","                                      evaluation_strategy='steps',\n","                                      eval_steps=1000,\n","                                      per_device_train_batch_size=2,\n","                                      per_device_eval_batch_size=2,\n","                                      warmup_steps=100,\n","                                      learning_rate=3e-5,\n","                                      weight_decay=0.01,\n","                                      logging_dir=f'./logs')\n","\n","trainer = Trainer(model=model, args=training_args,\n","                  train_dataset=train_data,\n","                  eval_dataset=val_data,\n","                  # This custom collate function is necessary\n","                  # to built batches of data\n","                  data_collator=lambda data: {'input_ids': torch.stack([f[0] for f in data]),\n","              'attention_mask': torch.stack([f[1] for f in data]),\n","              'labels': torch.stack([f[0] for f in data])})\n","\n","# Start training process!\n","trainer.train()\n","\n","\n","# Save model in the specified file path\n","trainer.save_model(\"drive/MyDrive/AICamp/models/yumyum_v2/\")\n","tokenizer.save_pretrained(\"drive/MyDrive/AICamp/models/yumyum_v2\")\n"],"metadata":{"id":"Ym8Ni5ilou6L","colab":{"base_uri":"https://localhost:8080/","height":225},"executionInfo":{"status":"ok","timestamp":1689289195583,"user_tz":240,"elapsed":409840,"user":{"displayName":"Rahul Reddy Vemparala","userId":"16442365183898895721"}},"outputId":"16e65ea7-ecb6-44e6-bb36-bbb2b1cd20d3"},"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","    <div>\n","      \n","      <progress value='2005' max='2005' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [2005/2005 06:46, Epoch 5/5]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Step</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>1000</td>\n","      <td>2.131500</td>\n","      <td>2.220036</td>\n","    </tr>\n","    <tr>\n","      <td>2000</td>\n","      <td>1.752500</td>\n","      <td>2.224234</td>\n","    </tr>\n","  </tbody>\n","</table><p>"]},"metadata":{}},{"output_type":"execute_result","data":{"text/plain":["('drive/MyDrive/AICamp/models/yumyum_v2/tokenizer_config.json',\n"," 'drive/MyDrive/AICamp/models/yumyum_v2/special_tokens_map.json',\n"," 'drive/MyDrive/AICamp/models/yumyum_v2/vocab.json',\n"," 'drive/MyDrive/AICamp/models/yumyum_v2/merges.txt',\n"," 'drive/MyDrive/AICamp/models/yumyum_v2/added_tokens.json')"]},"metadata":{},"execution_count":120}]},{"cell_type":"markdown","source":["## Checking Model Output"],"metadata":{"id":"1gHIDQpWo_uU"}},{"cell_type":"markdown","source":["## Note:\n","\n","return tensors: tensors are specific term in the transformers tokenizer, it also represent tokens ,and 'pt' stands for pytorch.\n","\n","1. generates: its the encoded text, we made using the transformers pretrained tokenizer we initialized above,\n","2. feeding to the saved model and uinsg the generate function to generate some text\n","3. max length : should be input lenght of tokens\n","4. num return sequences is number of sequences it should return as a reuslt, in our case number of sonnets(poems) it genertes."],"metadata":{"id":"DtmFV425w_3E"}},{"cell_type":"code","source":["generated = tokenizer(\"<|startoftext|>\", return_tensors=\"pt\").input_ids.cuda()\n","print(f\"generated: {generated[:1]}\")\n","sample_outputs = model.generate(generated, do_sample=True, top_k=20,\n","                                # bos_token='<|startoftext|>',\n","                                # eos_token='<|endoftext|>', pad_token='<|pad|>',\n","                                max_length=300, top_p=0.95, temperature=0.5, num_return_sequences=20)\n","for i, sample_output in enumerate(sample_outputs):\n","    print(\"{}: {}\".format(i, tokenizer.decode(sample_output, skip_special_tokens=True)))\n"],"metadata":{"id":"xl9xISOxo-2A"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Prompt the user for input\n","prompt = input(\"Ask for a Dish>> \")\n","\n","prompt = f\"give me recipe for {prompt}\"\n","\n","# Encode the prompt using the tokenizer\n","encoded_prompt = tokenizer.encode(prompt, return_tensors=\"pt\").cuda()\n","\n","# Generate the output based on the prompt\n","sample_outputs = model.generate(encoded_prompt, do_sample=True, top_k=20,\n","                                max_length=300, top_p=0.95, temperature=0.5, num_return_sequences=1)\n","\n","# Decode and print the generated outputs\n","for i, sample_output in enumerate(sample_outputs):\n","    decoded_output = tokenizer.decode(sample_output, skip_special_tokens=True)\n","    print(\"{}: {}\".format(i, decoded_output))\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"bmFpFW4MTJBi","executionInfo":{"status":"ok","timestamp":1689289853286,"user_tz":240,"elapsed":4785,"user":{"displayName":"Rahul Reddy Vemparala","userId":"16442365183898895721"}},"outputId":"d4654681-6d64-4104-e070-567f224f6408"},"execution_count":null,"outputs":[{"name":"stdout","output_type":"stream","text":["Ask for a Dish>> Microwave Lasagne\n"]},{"output_type":"stream","name":"stderr","text":["The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n","Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"]},{"output_type":"stream","name":"stdout","text":["0: give me recipe for Microwave Lasagne:\n"," Mix the milk sugar and butter in a bowl,\n"," Add the flour and mix well,\n"," Add the milk and mix well,\n"," Add the eggs and mix well,\n"," Add the flour mixture to the milk mixture and mix well,\n"," Pour the mixture into a bowl and stir with a fork until all the flour is added,\n"," Add the milk mixture to the egg mixture and mix well,\n"," Add the flour mixture to the milk mixture and mix well,\n"," Stir the dough until it is a ball,\n"," Divide the dough into 12 equal pieces and shape into a ball,\n"," Roll each piece into a circle and put in a greased baking dish,\n"," Cover with a towel and let rise in a warm place until doubled in size about 30 minutes,\n"," Bake at 350 degrees Fahrenheit for 25 minutes,\n"," Remove from the oven and cool on a wire rack,\n","NER:egg sugar flour milk butter eggs.\n"]}]},{"cell_type":"code","source":["import locale\n","locale.getpreferredencoding = lambda: \"UTF-8\"\n"],"metadata":{"id":"rSq0sIaT08Eb"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["! transformers-cli env"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"wTuMBlKNzkhj","executionInfo":{"status":"ok","timestamp":1689290562221,"user_tz":240,"elapsed":15946,"user":{"displayName":"Rahul Reddy Vemparala","userId":"16442365183898895721"}},"outputId":"c345362c-b030-47cf-eb5d-326722e17fd6"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["2023-07-13 23:22:22.848045: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n","WARNING:tensorflow:From /usr/local/lib/python3.10/dist-packages/transformers/commands/env.py:63: is_gpu_available (from tensorflow.python.framework.test_util) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Use `tf.config.list_physical_devices('GPU')` instead.\n","2023-07-13 23:22:34.160283: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:47] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.\n","\n","Copy-and-paste the text below in your GitHub issue and FILL OUT the two last points.\n","\n","- `transformers` version: 4.30.2\n","- Platform: Linux-5.15.109+-x86_64-with-glibc2.31\n","- Python version: 3.10.12\n","- Huggingface_hub version: 0.16.4\n","- Safetensors version: 0.3.1\n","- PyTorch version (GPU?): 2.0.1+cu118 (True)\n","- Tensorflow version (GPU?): 2.12.0 (True)\n","- Flax version (CPU?/GPU?/TPU?): 0.7.0 (gpu)\n","- Jax version: 0.4.13\n","- JaxLib version: 0.4.13\n","- Using GPU in script?: <fill in>\n","- Using distributed or parallel set-up in script?: <fill in>\n","\n"]}]},{"cell_type":"markdown","source":["## Upload model to huggingface"],"metadata":{"id":"DwSbqbWmME5l"}},{"cell_type":"code","source":["from huggingface_hub import notebook_login\n","\n","notebook_login()"],"metadata":{"id":"Nb5NjZnuMIYu","colab":{"base_uri":"https://localhost:8080/","height":145,"referenced_widgets":["ef39e42aa2384146b34a96be9a712e98","5adeae255d5144f5a842f0ca9bb6cff0","c1f89fe4232d4bba910ed265bd741052","8e5e029825384abeabf1cd52eb9a8270","5b24c7b9774446b1a984084b2c8acff4","98fcee6416b54a40b372ad3e42c5aa7a","27a850ff675c4885ba27da7ae801553e","8a6daa24e90a4fa1a8e43f1e31d72bf2","a0e13fd39a0f4b768f774b029973c794","0102249074a84c4583dca69669cad3e0","e5d8f6bd66724d30a0c6af47615d431b","a08c0d2e3c0b4bdd8e9aad1b90b79158","c4898b7f289f4a3f8614541865ce4304","dff12a6b17a248f3a0b056a752cd5216","3c661fdb450544b99db69e4d3cdecba7","819fc490dad74035850cef3b773ba67c","9228216d7352487d887ce639902c2810","2bf91bf4e93b47f6b5f565597bb99ffb","8e1c9e1331604714bf4e73fd4208ecd1","9f5a7776f2de4bcc994a4800b1a5a629","ee6c7596727b487f88d1038b04a5aa3c","575a096231244b37bab1ce86169d7195","ec635ae64252423b9b9b823501cdf0e9","e01cf4ce675e46559dfbc363975cc78d","34764913393644669fb1ecbffcfa6790","010d45f9e3124a5081db628ae9a03dce","3fded44e66db481882b1a6ac3671eab2","50e7f5af000f47a89e6db5f1ec700ffe","57db20e13dcb4e898a31d9f887a67756","c03759e6bf5742cd8fb1c932e75bc777","b4ac078ddcaa46e7a788808fa5ced40d","15aaed0aa87e40f5bf130d35b6118804"]},"executionInfo":{"status":"ok","timestamp":1689290567290,"user_tz":240,"elapsed":307,"user":{"displayName":"Rahul Reddy Vemparala","userId":"16442365183898895721"}},"outputId":"d67e9563-d974-4431-b792-0a99b75f5695"},"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/plain":["VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ef39e42aa2384146b34a96be9a712e98"}},"metadata":{}}]},{"cell_type":"code","source":["from huggingface_hub import HfApi\n","\n","api = HfApi()"],"metadata":{"id":"pzOXqg0LpbaI"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Create your repo first to upload the model\n","api.create_repo(repo_id=\"yum\")"],"metadata":{"id":"FTajb3uFpeuC","colab":{"base_uri":"https://localhost:8080/","height":34},"executionInfo":{"status":"ok","timestamp":1689290594905,"user_tz":240,"elapsed":5982,"user":{"displayName":"Rahul Reddy Vemparala","userId":"16442365183898895721"}},"outputId":"6c35aa19-18cf-44da-c144-5b88b962d044"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["RepoUrl('https://huggingface.co/callMeRover/yum', endpoint='https://huggingface.co', repo_type='model', repo_id='callMeRover/yum')"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":5}]},{"cell_type":"code","source":["# Upload your model to huggingface. You can clone the repo anytime to use the model.\n","import os\n","\n","model_pth = \"drive/MyDrive/AICamp/models/yumyum_v2\"\n","\n","files = os.listdir(model_pth)\n","print(files)\n","\n","for fi in files:\n","    print(os.path.join(model_pth, fi))\n","\n","    api.upload_file(\n","        path_or_fileobj=os.path.join(model_pth, fi),\n","        path_in_repo=fi,\n","        repo_id=\"callMeRover/yum\",\n","        repo_type=\"model\",\n","    )"],"metadata":{"id":"Bk0RPeLMphCy","colab":{"base_uri":"https://localhost:8080/","height":275,"referenced_widgets":["b23ae9782f144224a860736d70a59d35","38ce10041ebf4ff88cf1595006a69acd","7b51867613894df1a6664924a1724900","b0b0ac0c41fd4ab0a3510bfeec7bf68c","587f4c102ab245af999ef6558f21fddf","298dd37ec6464a30ba1376d1d2ab2308","b94b78db5e7f456a9ad923fc5d0c24f2","9935d3b74701443da6bf4d10432bc8e7","cbdac6bea0d646cca744234770fd6e96","6a02441956274469b86ab355a0207768","46bae19dee81449bb2969230c279acba","35c1ff6acfd1454a9437e763fe86c5b2","edb3ef2f57274369a8afa020ffc826ee","4f8e5163ba284a538c6a02027bdd3248","269fcea72f9547eaa074e6c581165c17","68185049c31e46fb8565bc3502ece635","0a38f9f1d01b42cb8925332bc4cea68b","e204716817eb43c0bbc377dd71b62c86","69148528a7a8477fada1804f0938191d","ad30201c2d00466dbe11c4fd8a7c1c24","ad9bb05246de465fb2a7287e95592650","a7ebac271ca34936b9e8de23239e8198"]},"executionInfo":{"status":"ok","timestamp":1689290681158,"user_tz":240,"elapsed":28473,"user":{"displayName":"Rahul Reddy Vemparala","userId":"16442365183898895721"}},"outputId":"9a7639e7-f028-4126-f5dd-e60eab87ede4"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["['config.json', 'generation_config.json', 'pytorch_model.bin', 'training_args.bin', 'tokenizer_config.json', 'special_tokens_map.json', 'added_tokens.json', 'vocab.json', 'merges.txt']\n","drive/MyDrive/AICamp/models/yumyum_v2/config.json\n","drive/MyDrive/AICamp/models/yumyum_v2/generation_config.json\n","drive/MyDrive/AICamp/models/yumyum_v2/pytorch_model.bin\n"]},{"output_type":"display_data","data":{"text/plain":["pytorch_model.bin:   0%|          | 0.00/551M [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b23ae9782f144224a860736d70a59d35"}},"metadata":{}},{"output_type":"stream","name":"stdout","text":["drive/MyDrive/AICamp/models/yumyum_v2/training_args.bin\n"]},{"output_type":"display_data","data":{"text/plain":["training_args.bin:   0%|          | 0.00/3.83k [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"35c1ff6acfd1454a9437e763fe86c5b2"}},"metadata":{}},{"output_type":"stream","name":"stdout","text":["drive/MyDrive/AICamp/models/yumyum_v2/tokenizer_config.json\n","drive/MyDrive/AICamp/models/yumyum_v2/special_tokens_map.json\n","drive/MyDrive/AICamp/models/yumyum_v2/added_tokens.json\n","drive/MyDrive/AICamp/models/yumyum_v2/vocab.json\n","drive/MyDrive/AICamp/models/yumyum_v2/merges.txt\n"]}]},{"cell_type":"code","source":["\"\"\"\n","parameters for inference api call\n","\"\"\"\n","## original\n","# parameters = {\n","#     \"top_k\" : 10,\n","#     \"max_length\": 100,\n","#     \"temperature\" : 0.2,\n","#     \"top_p\" : 0.22,\n","#     \"no_repeat_ngram_size\" : 3,\n","#     \"do_sample\": True,\n","#     }\n","\n","\n","#  WORKING PARAMETERS\n","# parameters = {\n","#     \"top_k\" : 20,\n","#     \"max_length\": 300,\n","#     \"temperature\" : 0.5,\n","#     \"top_p\" : 0.95,\n","#     \"do_sample\": True,\n","#     }\n","\n","\n","# options = {\"wait_for_model\": True\n","# }"],"metadata":{"id":"zZtnu8JJzy-5"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import requests\n","\n","\n","API_URL = \"https://api-inference.huggingface.co/models/callMeRover/yumYum\"\n","headers = {\"Authorization\": \"Bearer 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\"}\n","\n","\n","def query(payload):\n","    response = requests.post(API_URL, headers=headers, json=payload)\n","    return response.json()\n","\n","\n","output = query({\n","    \"inputs\": \"give me recipe for Butter Cookies: \",\n","    \"parameters\": parameters,\n","    \"options\" : options\n","})\n","\n","output"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"p84DCMB4z1gf","executionInfo":{"status":"ok","timestamp":1689290154512,"user_tz":240,"elapsed":2829,"user":{"displayName":"Rahul Reddy Vemparala","userId":"16442365183898895721"}},"outputId":"8da4d090-1668-4a78-b972-b3f2ccc4a4fc"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[{'generated_text': 'give me recipe for Butter Cookies: **************\\n\\nI use unsalted butter in this recipe to make a savory brownie. I use brown sugar in this recipe to make a savory brownie. I use shortening in this recipe to make a savory brownie. I use sour cream in this recipe to make a savory brownie.'}]"]},"metadata":{},"execution_count":146}]},{"cell_type":"code","source":["import requests\n","\n","API_URL = \"https://api-inference.huggingface.co/models/callMeRover/yum\"\n","headers = {\"Authorization\": \"Bearer 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\"}\n","\n","\n","def query(payload):\n","    response = requests.post(API_URL, headers=headers, json=payload)\n","    return response.json()\n","\n","dish_name = input(\"Enter the dish name: \")\n","\n","prompt = f\"Give me recipe for {dish_name}:\"\n","\n","parameters = {\n","    \"do_sample\": True,\n","    \"max_length\": 400,\n","    \"top_k\": 30,\n","    \"top_p\": 0.95,\n","    \"temperature\": 0.5,\n","    \"num_return_sequences\": 1,\n","}\n","\n","\n","options = {\"wait_for_model\": True\n","}\n","\n","\n","output = query({\n","    \"inputs\": prompt,\n","    \"parameters\": parameters,\n","    \"options\" : options\n","})\n","\n","\n"],"metadata":{"id":"LeKu7k2-gf5I"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["result = output[0]['generated_text'].split('\\n')\n","result"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"8Qs-OzDmjfdt","executionInfo":{"status":"ok","timestamp":1689797940819,"user_tz":240,"elapsed":6,"user":{"displayName":"Rahul Reddy Vemparala","userId":"16442365183898895721"}},"outputId":"fd8f216a-a4ef-4d0e-e8fe-eaeb98dd84e8"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["['Give me recipe for Misericordia Crabcakes:',\n"," ' Crust:',\n"," ' Buttercup,',\n"," ' In a large bowl combine flour baking powder sugar salt and cinnamon  Stir until well mixed  Add flour mixture to shell and mix well  Place crabmeat in a large bowl and cover with plastic wrap  Repeat the process with remaining ingredients  Cover and chill for 1 hour  Preheat oven to 375 degrees FahrenheitF  Bake crabcakes for 20 minutes or until golden brown and crisp  Cool on a wire rack for 5 minutes  Cool completely  Cut into 1-inch slices  Cut into 1-inch pieces and serve with whipped cream or with a small dollop of whipped cream or with a small dollop of whipped cream  Makes 4 servings,',\n"," 'NER:sugar baking powder sugar cinnamon crabmeat flour salt.']"]},"metadata":{},"execution_count":39}]},{"cell_type":"code","source":["type(result)"],"metadata":{"id":"FEm5AqjgnE9A","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1689780965327,"user_tz":240,"elapsed":333,"user":{"displayName":"Rahul Reddy Vemparala","userId":"16442365183898895721"}},"outputId":"ecc7793e-4572-4a94-db8c-8ba3549c9a83"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["list"]},"metadata":{},"execution_count":6}]},{"cell_type":"code","source":[],"metadata":{"id":"rPghAJ-kyaaV"},"execution_count":null,"outputs":[]}]}